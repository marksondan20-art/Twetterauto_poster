import os, re, json, time, hashlib, pathlib, random, argparse, threading
from datetime import datetime, timedelta
from zoneinfo import ZoneInfo
import feedparser
import requests
import tweepy
from tweepy import Client
from flask import Flask

# ========================
# Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø¹Ø§Ù…Ø©
# ========================
BAGHDAD_TZ = ZoneInfo("Asia/Baghdad")
POST_TIMES_LOCAL = ["12:00", "19:00"]  # Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù†Ø´Ø± Ø§Ù„ÙŠÙˆÙ…ÙŠØ©
POLL_EVERY_MIN = 30  # ÙØ­Øµ RSS ÙƒÙ„ X Ø¯Ù‚ÙŠÙ‚Ø©
RESURFACE_EVERY_HOURS = 72  # Ø¥Ø­ÙŠØ§Ø¡ ÙƒÙ„ 72 Ø³Ø§Ø¹Ø©
MAX_NEW_PER_RUN = 3  # Ø­Ø¯ Ø£Ù‚ØµÙ‰ Ù†Ø´Ø± Ø¬Ø¯ÙŠØ¯ ÙÙŠ Ø§Ù„ØªØ´ØºÙŠÙ„ Ø§Ù„ÙˆØ§Ø­Ø¯

SITE_URL = os.environ.get("SITE_URL", "https://loadingapk.online")
YOUTUBE_URL = "https://www.youtube.com/@-Muhamedloading"

STATE_JSON = pathlib.Path("posts.json")
RESURFACE_TS = pathlib.Path("last_resurface.txt")
SEEN_LINKS = pathlib.Path("seen_links.txt")

# Ù‡Ø§Ø´ØªØ§ØºØ§Øª Ø«Ø§Ø¨ØªØ© (ØªØ£ÙƒØ¯ ÙˆØ¬ÙˆØ¯ #Ù„ÙˆØ¯ÙŠÙ†Øº)
HASHTAGS = "#Ù„ÙˆØ¯ÙŠÙ†Øº #Ù…Ù‚Ø§Ù„Ø§Øª #Ø£Ø¨Ø­Ø§Ø« #ØªØ§Ø±ÙŠØ® #ØªÙ‚Ù†ÙŠØ©"

# ========================
# Ù…ÙØ§ØªÙŠØ­ X (ØªÙˆÙŠØªØ±)
# ========================
API_KEY = os.environ["TW_API_KEY"]
API_KEY_SECRET = os.environ["TW_API_KEY_SECRET"]
ACCESS_TOKEN = os.environ["TW_ACCESS_TOKEN"]
ACCESS_TOKEN_SECRET = os.environ["TW_ACCESS_TOKEN_SECRET"]
BEARER_TOKEN = os.environ["TW_BEARER_TOKEN"]

# Ø¹Ù…ÙŠÙ„ v2 Ù„Ù„ØªØºØ±ÙŠØ¯
client = Client(bearer_token=BEARER_TOKEN,
                consumer_key=API_KEY,
                consumer_secret=API_KEY_SECRET,
                access_token=ACCESS_TOKEN,
                access_token_secret=ACCESS_TOKEN_SECRET,
                wait_on_rate_limit=True)

# Ø¹Ù…ÙŠÙ„ v1.1 Ù„Ø±ÙØ¹ Ø§Ù„ÙˆØ³Ø§Ø¦Ø·
auth = tweepy.OAuth1UserHandler(API_KEY, API_KEY_SECRET, ACCESS_TOKEN,
                                ACCESS_TOKEN_SECRET)
api_v1 = tweepy.API(auth, wait_on_rate_limit=True)

RSS = os.environ.get("BLOG_RSS_URL",
                     "https://loadingapk.online/feeds/posts/default?alt=rss")

# ØªÙ‡ÙŠØ¦Ø© Ù…Ù„ÙØ§Øª Ø§Ù„Ø­Ø§Ù„Ø©
if not STATE_JSON.exists(): STATE_JSON.write_text("[]", encoding="utf-8")
if not SEEN_LINKS.exists(): SEEN_LINKS.write_text("", encoding="utf-8")

# ========================
# Ø£Ø¯ÙˆØ§Øª Ù…Ø³Ø§Ø¹Ø¯Ø©
# ========================
BAD_PHRASES = [
    r'Ø§Ù„Ù…ØµØ¯Ø±\s*[:\-â€“]?\s*pexels', r'pexels', r'pixabay', r'unsplash',
    r'Image\s*\(forced.*?\)', r'\bsource\b.*', r'Ø­Ù‚ÙˆÙ‚.*?Ø§Ù„ØµÙˆØ±Ø©', r'ØµÙˆØ±Ø©\s+Ù…Ù†'
]
BAD_RE = re.compile("|".join(BAD_PHRASES), re.IGNORECASE)

IMG_RE = re.compile(r'<img[^>]+src=["\']([^"\']+)["\']', re.IGNORECASE)


def now_local():
    return datetime.now(BAGHDAD_TZ)


def load_json():
    try:
        return json.loads(STATE_JSON.read_text(encoding="utf-8"))
    except:
        return []


def save_json(items):
    STATE_JSON.write_text(json.dumps(items, ensure_ascii=False, indent=2),
                          encoding="utf-8")


def load_seen():
    return set(l.strip()
               for l in SEEN_LINKS.read_text(encoding="utf-8").splitlines()
               if l.strip())


def save_seen(seen: set):
    SEEN_LINKS.write_text("\n".join(sorted(seen)), encoding="utf-8")


def sha(link: str) -> str:
    return hashlib.sha1(link.encode("utf-8")).hexdigest()


def clean_html(s: str) -> str:
    if not s: return ""
    s = re.sub(r"<[^>]+>", " ", s)  # Ø¥Ø²Ø§Ù„Ø© Ø§Ù„ÙˆØ³ÙˆÙ…
    s = re.sub(BAD_RE, " ", s)  # Ø­Ø°Ù Ø£Ø³Ø·Ø± Ø§Ù„Ù…ØµØ§Ø¯Ø±/Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª
    s = re.sub(r"\s+", " ", s).strip()
    return s


def shorten(s: str, n: int) -> str:
    return s if len(s) <= n else s[:max(0, n - 1)].rstrip() + "â€¦"


def to_question(title: str, summary: str) -> str:
    """ÙŠØµÙŠØº Ø³Ø¤Ø§Ù„Ù‹Ø§ ØªØ´ÙˆÙŠÙ‚ÙŠÙ‹Ø§ Ù…Ù† Ø§Ù„Ø¹Ù†ÙˆØ§Ù†/Ø§Ù„Ù…Ù„Ø®Øµ."""
    starts = [
        "Ù‡Ù„ ÙŠÙ…ÙƒÙ† Ø£Ù†", "Ø¥Ù„Ù‰ Ø£ÙŠ Ø­Ø¯ ÙŠÙ…ÙƒÙ† Ø£Ù†", "Ù…Ø§ Ø§Ù„Ø°ÙŠ ÙŠØ¬Ø¹Ù„", "ÙƒÙŠÙ ØªØºÙŠÙ‘Ø±",
        "Ù…ØªÙ‰ ÙŠØµØ¨Ø­", "Ù„Ù…Ø§Ø°Ø§ Ù‚Ø¯ ÙŠÙƒÙˆÙ†", "Ù‡Ù„ ÙØ¹Ù„Ø§Ù‹"
    ]
    start = random.choice(starts)
    base = title
    if len(base) < 40 and summary:
        base = f"{title}: {summary}"
    base = re.sub(r"[\.!\u061F]+$", "", base).strip()
    return shorten(f"{start} {base}ØŸ", 140)


def compose_tweet(title: str, summary: str, url: str) -> str:
    """
    ÙŠØ¨Ù†ÙŠ ØªØºØ±ÙŠØ¯Ø© Ù…ØªØ¹Ø¯Ø¯Ø© Ø§Ù„Ø£Ø³Ø·Ø± (â‰¥ 3 Ø£Ø³Ø·Ø±):
    1) Ø³Ø¤Ø§Ù„ ØªØ´ÙˆÙŠÙ‚ÙŠ
    2) Ù‡Ø§Ø´ØªØ§ØºØ§Øª ØªØ´Ù…Ù„ #Ù„ÙˆØ¯ÙŠÙ†Øº
    3) Ø±Ø§Ø¨Ø· Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ù„Ù‚Ø±Ø§Ø¡Ø© Ø§Ù„Ù…Ù†Ø´ÙˆØ±
    4) Ø±Ø§Ø¨Ø· Ø§Ù„ÙŠÙˆØªÙŠÙˆØ¨ (Ù‚Ø§Ø¨Ù„ Ù„Ù„Ù†Ù‚Ø±) â€” ÙŠÙØ­Ø°Ù ÙÙ‚Ø· Ø¥Ø°Ø§ ØªØ¹Ø°Ù‘Ø± Ø§Ù„Ø·ÙˆÙ„
    """
    q = to_question(title, summary)

    line1 = q
    line2 = HASHTAGS
    line3 = f"ğŸ”— Ø§Ù‚Ø±Ø£ Ù…Ù† Ø§Ù„Ù…ÙˆÙ‚Ø¹: {url}"
    line4 = f"ğŸ¬ ÙŠÙˆØªÙŠÙˆØ¨: {YOUTUBE_URL}"

    # Ø­Ø§ÙˆÙ„ ØªØ¶Ù…ÙŠÙ† 4 Ø£Ø³Ø·Ø±ØŒ Ø«Ù… Ù‚Ù„Ù‘Ù… ØªØ¯Ø±ÙŠØ¬ÙŠÙ‹Ø§ Ù…Ø¹ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ â‰¥ 3 Ø£Ø³Ø·Ø±
    body4 = "\n".join([line1, line2, line3, line4])
    if len(body4) <= 280: return body4

    body3 = "\n".join([line1, line2, line3])
    if len(body3) <= 280: return body3

    for qlen in (120, 110, 100, 90, 80, 70, 60):
        body_try = "\n".join([shorten(line1, qlen), line2, line3])
        if len(body_try) <= 280:
            return body_try

    mini_tags = "#Ù„ÙˆØ¯ÙŠÙ†Øº #Ù…Ù‚Ø§Ù„Ø§Øª"
    body_mini = "\n".join([shorten(line1, 60), mini_tags, line3])
    if len(body_mini) <= 280: return body_mini

    return f"{shorten(q, 60)}\n#Ù„ÙˆØ¯ÙŠÙ†Øº\n{line3}"


def find_image_url(entry) -> str | None:
    """ÙŠØ­Ø§ÙˆÙ„ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø£ÙˆÙ‘Ù„ ØµÙˆØ±Ø© Ù…Ù† RSS (media_content/thumbnail Ø£Ùˆ content HTML)."""
    # 1) media:content / media:thumbnail
    for key in ("media_content", "media_thumbnail"):
        if entry.get(key):
            try:
                url = entry[key][0].get("url")
                if url and url.startswith(("http://", "https://")):
                    return url
            except Exception:
                pass
    # 2) Ù…Ù† content/summary Ø¨Ù€ <img src="...">
    html_blob = entry.get("content", [{}])[0].get("value") if entry.get(
        "content") else entry.get("summary", "")
    if html_blob:
        m = IMG_RE.search(html_blob)
        if m:
            url = m.group(1)
            if url.startswith("//"): url = "https:" + url
            if url.startswith(
                ("http://", "https://")) and not url.startswith("data:"):
                return url
    return None


def download_image(url: str,
                   timeout=10,
                   max_bytes=5 * 1024 * 1024) -> str | None:
    """ÙŠØ­Ù…Ù‘Ù„ Ø§Ù„ØµÙˆØ±Ø© Ø¥Ù„Ù‰ Ù…Ù„Ù Ù…Ø¤Ù‚Øª ÙˆÙŠØ±Ø¬Ø¹ Ø§Ù„Ù…Ø³Ø§Ø±Ø› ÙˆØ¥Ù„Ø§ None."""
    try:
        headers = {"User-Agent": "Mozilla/5.0 (compatible; LoadingAPKBot/1.0)"}
        with requests.get(url, headers=headers, stream=True,
                          timeout=timeout) as r:
            r.raise_for_status()
            ctype = r.headers.get("Content-Type", "").lower()
            if not any(x in ctype for x in
                       ["image/jpeg", "image/png", "image/webp", "image/jpg"]):
                # Ù†Ø¬Ø±Ø¨ Ø±ØºÙ… Ø°Ù„Ùƒ Ø¥Ù† Ù„Ù… ÙŠÙØ¹Ù„Ù† Ø§Ù„Ù†ÙˆØ¹
                pass
            # Ø­ÙØ¸ Ø¥Ù„Ù‰ /tmp
            ext = ".jpg"
            if "png" in ctype: ext = ".png"
            elif "webp" in ctype: ext = ".webp"
            path = f"/tmp/ldg_{int(time.time())}{ext}"
            size = 0
            with open(path, "wb") as f:
                for chunk in r.iter_content(8192):
                    if not chunk: continue
                    size += len(chunk)
                    if size > max_bytes:
                        f.close()
                        try:
                            os.remove(path)
                        except:
                            pass
                        return None
                    f.write(chunk)
            return path
    except Exception as e:
        print("[IMG] ÙØ´Ù„ ØªÙ†Ø²ÙŠÙ„ Ø§Ù„ØµÙˆØ±Ø©:", e)
        return None


def upload_media_get_id(img_path: str) -> int | None:
    """ÙŠØ±ÙØ¹ Ø§Ù„ØµÙˆØ±Ø© Ø¹Ø¨Ø± v1.1 ÙˆÙŠØ¹ÙŠØ¯ media_idØ› Ø£Ùˆ None."""
    try:
        media = api_v1.media_upload(filename=img_path)
        return media.media_id
    except Exception as e:
        print("[IMG] ÙØ´Ù„ Ø§Ù„Ø±ÙØ¹:", e)
        return None


def fetch_entries():
    feed = feedparser.parse(RSS)
    entries = []
    for e in feed.entries:
        title = (e.get("title") or "").strip()
        link = (e.get("link") or "").strip()
        summary = clean_html(
            e.get("summary", "") or (e.get("content", [{}])[0].get("value")
                                     if e.get("content") else ""))
        entries.append({
            "title": title,
            "link": link,
            "summary": summary,
            "raw": e
        })
    return entries


# ========================
# Ù†Ø´Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ø¬Ø¯ÙŠØ¯Ø© (Ù…Ø¹ ØµÙˆØ±Ø© Ù„Ùˆ Ø£Ù…ÙƒÙ†)
# ========================
def post_new_articles(limit=MAX_NEW_PER_RUN):
    entries = fetch_entries()
    if not entries:
        print("[RSS] Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¹Ù†Ø§ØµØ±.")
        return 0

    state = load_json()
    posted_pids = {x["pid"] for x in state}
    seen = load_seen()

    published = 0
    for item in entries[:10]:  # Ø§Ù„Ø£Ø­Ø¯Ø« Ø£ÙˆÙ„Ø§Ù‹
        pid = sha(item["link"])
        if pid in posted_pids or item["link"] in seen:
            continue

        tweet = compose_tweet(item["title"], item["summary"], item["link"])

        media_ids = None
        try:
            img_url = find_image_url(item["raw"])
            if img_url:
                img_path = download_image(img_url)
                if img_path:
                    mid = upload_media_get_id(img_path)
                    if mid:
                        media_ids = [mid]
                        print("[IMG] Ø£ÙØ±ÙÙ‚Øª ØµÙˆØ±Ø©:", img_url)
        except Exception as e:
            print("[IMG] ØªØ®Ø·Ù‘ÙŠ Ø§Ù„ØµÙˆØ±Ø© Ø¨Ø³Ø¨Ø¨ Ø®Ø·Ø£:", e)

        if media_ids:
            resp = client.create_tweet(text=tweet, media_ids=media_ids)
        else:
            resp = client.create_tweet(text=tweet)

        tid = resp.data["id"]
        print("[NEW] ØªÙ… Ø§Ù„Ù†Ø´Ø±:", tid, "â†’", item["link"])

        state.append({
            "pid": pid,
            "title": item["title"],
            "link": item["link"],
            "tweet_id": tid,
            "posted_at": int(time.time())
        })
        save_json(state)

        seen.add(item["link"])
        save_seen(seen)
        published += 1
        if published >= limit: break

    if published == 0: print("[NEW] Ù„Ø§ Ø¬Ø¯ÙŠØ¯ Ù„Ù„Ù†Ø´Ø±.")
    return published


# ========================
# Ø¥Ø­ÙŠØ§Ø¡ ØªÙ„Ù‚Ø§Ø¦ÙŠ ÙƒÙ„ 72 Ø³Ø§Ø¹Ø©
# ========================
def maybe_resurface():
    now = int(time.time())
    last = 0
    if RESURFACE_TS.exists():
        try:
            last = int(RESURFACE_TS.read_text().strip() or "0")
        except:
            last = 0

    if now - last < RESURFACE_EVERY_HOURS * 3600:
        print("[RESURFACE] Ù„Ù… ÙŠØ­Ù† Ø§Ù„ÙˆÙ‚Øª Ø¨Ø¹Ø¯.")
        return None

    state = load_json()
    if len(state) < 2:
        print("[RESURFACE] Ø§Ù„Ø£Ø±Ø´ÙŠÙ ØµØºÙŠØ±.")
        RESURFACE_TS.write_text(str(now))
        return None

    cand = random.choice(state[:-1])  # Ø§Ø³ØªØ¨Ø¹Ø¯ Ø§Ù„Ø£Ø­Ø¯Ø«
    quote_text = random.choice([
        "ØªØ°ÙƒÙŠØ± Ù…Ù‡Ù… Ù…Ù† Ø£Ø±Ø´ÙŠÙÙ†Ø§ ğŸ“š", "Ø¹ÙˆØ¯Ø© Ù„ÙˆØ§Ø­Ø¯Ø© Ù…Ù† Ù‚Ø±Ø§Ø¡Ø§ØªÙ†Ø§ Ø§Ù„Ù…ÙØ¶Ù„Ø© ğŸ”",
        "Ù‡Ù„ ÙØ§ØªØªÙƒ Ù‡Ø°Ù‡ØŸ ğŸ‘‡"
    ])
    resp = client.create_tweet(text=quote_text,
                               quote_tweet_id=cand["tweet_id"])
    print("[RESURFACE] Ø§Ù‚ØªØ¨Ø§Ø³:", resp.data["id"], "â†", cand["tweet_id"])
    RESURFACE_TS.write_text(str(now))
    return resp.data["id"]


# ========================
# Ø¬Ø¯ÙˆÙ„Ø© Ø¯Ø§Ø®Ù„ÙŠØ© (Ø¯ÙŠÙ…ÙˆÙ†)
# ========================
def parse_times_local(times_list):
    return [(int(t.split(":")[0]), int(t.split(":")[1])) for t in times_list]


def next_fire_after(now_dt, hh, mm):
    fire = now_dt.replace(hour=hh, minute=mm, second=0, microsecond=0)
    if fire <= now_dt: fire += timedelta(days=1)
    return fire


def run_daemon():
    post_slots = parse_times_local(POST_TIMES_LOCAL)
    next_runs = {
        (h, m): next_fire_after(now_local(), h, m)
        for (h, m) in post_slots
    }
    print("[DAEMON] Ø¨Ø¯Ø£ Ø§Ù„Ø¹Ù…Ù„. ÙØ­Øµ ÙƒÙ„", POLL_EVERY_MIN, "Ø¯Ù‚ÙŠÙ‚Ø©. Ø£ÙˆÙ‚Ø§Øª:",
          POST_TIMES_LOCAL)

    # ØªØ´ØºÙŠÙ„ Ø£ÙˆÙ„ÙŠ
    post_new_articles()
    maybe_resurface()

    last_poll = datetime.min.replace(tzinfo=BAGHDAD_TZ)
    while True:
        now = now_local()

        # ÙØ­Øµ RSS Ø¯ÙˆØ±ÙŠ
        if (now - last_poll) >= timedelta(minutes=POLL_EVERY_MIN):
            print("[POLL]", now.strftime("%Y-%m-%d %H:%M"))
            post_new_articles()
            maybe_resurface()
            last_poll = now

        # ØªÙ†ÙÙŠØ° Ø¹Ù†Ø¯ Ø§Ù„Ø£ÙˆÙ‚Ø§Øª Ø§Ù„Ù…Ø­Ø¯Ø¯Ø©
        for (h, m), fire in list(next_runs.items()):
            if now >= fire:
                print(
                    f"[SLOT {h:02d}:{m:02d}] ÙˆÙ‚Øª Ø§Ù„Ù†Ø´Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯ â€” Ù…Ø­Ø§ÙˆÙ„Ø© Ù†Ø´Ø± Ø¬Ø¯ÙŠØ¯."
                )
                post_new_articles(limit=MAX_NEW_PER_RUN)
                next_runs[(h, m)] = next_fire_after(now, h, m)

        time.sleep(20)


# ========================
# Ø®Ø§Ø¯Ù… Flask Ù„Ù„Ø¥Ø¨Ù‚Ø§Ø¡ Ø¹Ù„Ù‰ Replit Ù†Ø´Ø·Ù‹Ø§
# ========================
app = Flask("keep_alive")


@app.get("/")
def home():
    return "Bot is running âœ…", 200


def start_web():
    app.run(host="0.0.0.0", port=8080)


# ========================
# Ø§Ù„ØªØ´ØºÙŠÙ„
# ========================
def main():
    parser = argparse.ArgumentParser(
        description="Twitter auto poster for LoadingAPK")
    parser.add_argument("--daemon",
                        action="store_true",
                        help="ØªØ´ØºÙŠÙ„ Ø¯Ø§Ø¦Ù… Ù…Ø¹ Ø¬Ø¯ÙˆÙ„Ø© Ø¯Ø§Ø®Ù„ÙŠØ©")
    args = parser.parse_args()

    if args.daemon:
        threading.Thread(target=start_web, daemon=True).start()
        run_daemon()
    else:
        posted = post_new_articles(limit=MAX_NEW_PER_RUN)
        maybe_resurface()
        print("[DONE] ØªÙ…Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. Ù†ÙØ´Ø±:", posted)


if __name__ == "__main__":
    main()
